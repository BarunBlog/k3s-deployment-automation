name: Apply Kubernetes Manifests

on:
  workflow_run:
    workflows: [ "Deploy Auto Scaler" ]
    types:
      - completed

  workflow_dispatch:

jobs:
  apply_manifests:
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}

    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-1

      - name: Pulumi login
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
        run: pulumi login

      - name: Pulumi refresh - Common
        run: pulumi refresh --yes --cwd infra/common --stack dev

      - name: Pulumi refresh - Master
        run: pulumi refresh --yes --cwd infra/k3s-cluster/master --stack dev

      - name: Save Pulumi outputs
        id: pulumi_outputs
        run: |
          GIT_RUNNER_IP=$(pulumi stack output git_runner_public_ip --cwd infra/k3s-cluster/master -s dev)
          MASTER_NODE_IP=$(pulumi stack output master_private_ip --cwd infra/k3s-cluster/master -s dev)
          ALB_DNS=$(pulumi stack output alb_dns --cwd infra/k3s-cluster/master -s dev)
          SQS_URL=$(pulumi stack output nth_queue_url --cwd infra/k3s-cluster/worker -s dev)
          
          echo "GIT_RUNNER_IP=$GIT_RUNNER_IP" >> $GITHUB_ENV
          echo "MASTER_NODE_IP=$MASTER_NODE_IP" >> $GITHUB_ENV
          echo "ALB_DNS=$ALB_DNS" >> $GITHUB_ENV
          echo "SQS_URL=$SQS_URL" >> $GITHUB_ENV
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      - name: Set up SSH agent
        uses: webfactory/ssh-agent@v0.5.3
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Inject ALB DNS into Manifests
        run: |
          # Use sed to replace the placeholder in all ingress files inside the directory
          find ./k8s-manifests/ingress -type f -name "*.yaml" -exec sed -i "s/{{ alb_dns_name }}/${{ env.ALB_DNS }}/g" {} +
          
          # Verify replacement (optional debug)
          echo "Verifying DNS injection in prometheus-ingress.yaml:"
          grep "host:" ./k8s-manifests/ingress/prometheus-ingress.yaml

      - name: Transfer Manifests to Git-Runner
        run: |
          # Sync the merged k8s-manifests folder to the runner instance
          scp -o StrictHostKeyChecking=no -r ./k8s-manifests ubuntu@${{ env.GIT_RUNNER_IP }}:~/

      - name: SSH into GitHub Runner and deploy manifests
        run: |
          ssh -o StrictHostKeyChecking=no ubuntu@${{ env.GIT_RUNNER_IP }} << 'EOF'
            
            # Print commands to debug issues
            set -ex
          
            # Define cleanup function
            cleanup() {
              echo "Cleaning up kubeconfig..."
              rm -f ~/.kube/config
            }
          
            # Execute cleanup on exit (normal or error)
            trap cleanup EXIT
          
            # Install kubectl (Needed for raw manifests and Helm)
            if ! command -v kubectl &> /dev/null; then
              echo "Installing kubectl..."
              curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
              sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            fi
            
            # Fix Key Permissions
            chmod 600 ~/.ssh/my-key-pair.pem
            
            # Get Kubeconfig from Master Node
            # We use SSH from the Runner to the Master to grab the file
            ssh -n -o StrictHostKeyChecking=no -i ~/.ssh/my-key-pair.pem ubuntu@${{ env.MASTER_NODE_IP }} "sudo cat /etc/rancher/k3s/k3s.yaml" > k3s-temp.yaml
            
            # Update the API Server address in the config
            # Replace 127.0.0.1 with the actual Private IP of the Master Node
            sed -i "s/127.0.0.1/${{ env.MASTER_NODE_IP }}/g" k3s-temp.yaml
            
            # Move it to the default location for kubectl/helm
            mkdir -p ~/.kube
            mv k3s-temp.yaml ~/.kube/config
            chmod 600 ~/.kube/config
            
            # Verify connection
            kubectl get nodes
          
            # Change directory to manifests 
            cd ~/k8s-manifests

            # Install Helm if not installed
            if ! command -v helm &> /dev/null
            then
              echo "Installing Helm..."
              curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
            fi

            # Add ingress-nginx repo
            helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx || true
            helm repo update

            # Apply ingress-nginx
            helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
              --namespace ingress-nginx --create-namespace \
              -f helm/ingress-nginx/values-prod.yaml
          
            # ----------------------------------- Install or Upgrade Prometheus ----------------------------------------
            
            # Add the prometheus repository
            helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
            helm repo update
            
            helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
              --namespace monitoring --create-namespace \
              --set prometheus.prometheusSpec.retention=7d \
              --set prometheus.prometheusSpec.externalUrl="http://${ALB_DNS}/prometheus" \
              --set prometheus.prometheusSpec.routePrefix="/prometheus" \
              --set prometheus.service.type=NodePort \
              --set prometheus.service.nodePort=30090 \
              --wait
          
            # ------------------------------------ Prometheus Installation end -----------------------------------------

            # Create namespaces
            kubectl create namespace ${{ secrets.ENVIRONMENT }} --dry-run=client -o yaml | kubectl apply -f -
          
            # Set the current context to the desired namespace
            kubectl config set-context --current --namespace=${{ secrets.ENVIRONMENT }}

            # Apply ConfigMaps
            # kubectl apply -f config/

            # Create secret dynamically
            kubectl create secret generic rabbitmq-secret -n development \
              --from-literal=username=${{ secrets.RABBITMQ_DEFAULT_USER }} \
              --from-literal=password=${{ secrets.RABBITMQ_DEFAULT_PASS }} \
              --from-literal=RABBITMQ_DEFAULT_USER=${{ secrets.RABBITMQ_DEFAULT_USER }} \
              --from-literal=RABBITMQ_DEFAULT_PASS=${{ secrets.RABBITMQ_DEFAULT_PASS }} \
              --from-literal=RABBITMQ_HOST=${{ secrets.RABBITMQ_HOST }} \
              --from-literal=RABBITMQ_PORT=${{ secrets.RABBITMQ_PORT }} \
              --from-literal=default_user.conf="default_user = ${{ secrets.RABBITMQ_DEFAULT_USER }}
            default_pass = ${{ secrets.RABBITMQ_DEFAULT_PASS }}" \
              --dry-run=client -o yaml | kubectl apply -f -
            
            # install EBS CSI driver using HELM
            helm repo add aws-ebs-csi-driver \
              https://kubernetes-sigs.github.io/aws-ebs-csi-driver
            
            helm upgrade --install aws-ebs-csi-driver \
              aws-ebs-csi-driver/aws-ebs-csi-driver \
              --namespace kube-system \
              --set controller.region=ap-southeast-1 \
              --wait
            
            # ------------------------- Install AWS Node Termination Handler -------------------------------------------
            # Add the EKS charts repository
            helm repo add eks https://aws.github.io/eks-charts
            helm repo update
          
            # Get the SQS Queue URL from Pulumi first
            SQS_URL=$(pulumi stack output nth_queue_url --cwd infra/common -s dev)
            
            echo "Installing AWS Node Termination Handler..."
            helm upgrade --install aws-node-termination-handler eks/aws-node-termination-handler \
              --namespace kube-system \
              --set enableSpotInterruptionDraining=true \
              --set enableRebalanceMonitoring=true \
              --set queueUrl=$SQS_URL \
              --set enableScheduledEventDraining=true \
              --wait
            
            # ----------------------------------------------------------------------------------------------------------
          
            # Creating the Storage class to store data from rabbitmq
            kubectl apply -f storage/
          
            # ----------------------------------- Install RabbitMQ Operator --------------------------------------------
            echo "Installing RabbitMQ Cluster Operator..."
            kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml
            
            # Wait for operator to be ready
            kubectl rollout status deployment/rabbitmq-cluster-operator -n rabbitmq-system --timeout=120s
            # ----------------------------------------------------------------------------------------------------------

            # Apply RabbitMQ
            kubectl apply -f rabbitmq/cluster

            # Apply Services (recursive)
            kubectl apply -f services/ -R
            
            # Apply Ingresses
            kubectl apply -f ingress/
          
            # Delete the key once done
            rm -rf ~/.kube/config

            echo "All manifests applied successfully."
          EOF